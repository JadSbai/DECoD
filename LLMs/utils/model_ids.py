mixtral_optimised = "ybelkada/Mixtral-8x7B-Instruct-v0.1-bnb-4bit" # Still too big, needs to be cpu offloaded
phixtral = "mlabonne/phixtral-2x2_8" # Sucks
big_phixtral = "mlabonne/phixtral-4x2_8" # Sucks
neural_beagle = "mlabonne/NeuralBeagle14-7B" # Havent tried yet
mistral = "mistralai/Mistral-7B-Instruct-v0.2" # Try again
phi2 = "microsoft/phi-2" # Returns something
jellyfish = "NECOUDBFM/Jellyfish" # 13B parameters (26GB), try with vllm library. 
tableLlama = "osunlp/TableLlama" # No tokenizer
phi3 = "microsoft/Phi-3-mini-128k-instruct" # No tokenizer